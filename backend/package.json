{
	"name": "local-llm-backend",
	"version": "1.0.0",
	"description": "Backend server for local LLM chat interface using llama.cpp",
	"main": "src/server.js",
	"scripts": {
		"start": "node src/server.js",
		"dev": "nodemon src/server.js",
		"setup-models": "node src/setup-models.js"
	},
	"dependencies": {
		"express": "^4.18.2",
		"cors": "^2.8.5",
		"node-llama-cpp": "^3.0.1",
		"dotenv": "^16.5.0",
		"multer": "^1.4.5-lts.1",
		"axios": "^1.7.2"
	},
	"devDependencies": {
		"nodemon": "^3.0.1"
	},
	"engines": {
		"node": ">=18.0.0"
	}
}